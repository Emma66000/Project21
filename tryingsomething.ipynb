{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "  \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"castorini/t5-base-canard\")\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"castorini/t5-base-canard\")\n",
    "\n",
    "#help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-322fae36e1c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Who\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_ids\u001b[0m  \u001b[1;31m# Batch size 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Who\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Loading import load_queries\n",
    "queries = load_queries(\"data/2020_manual_evaluation_topics_v1.0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=1)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.004352569580078, 5.608213424682617, -4.102016925811768]\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 6.0044, -5.1765],\n",
      "        [ 5.6082, -4.6630],\n",
      "        [-4.1020,  3.7822]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "string1 = 'query: What is a corporation? passage:'\n",
    "answer1=  'A company is incorporated in a specific nation, often within the bounds of a smaller subset of that nation, such as a state or province. The corporation is then governed by the laws of incorporation in that state. A corporation may issue stock, either private or public, or may be classified as a non-stock corporation. If stock is issued, the corporation will usually be governed by its shareholders,either directly or indirectly.'\n",
    "string2 = 'query: This is coherent text?'\n",
    "answer2 = 'passage: odiuu nsd89fu dsutrfh e9r8ufh sd9pf8uyhs d9pfu7yes '\n",
    "string3 = 'query: How frustrating has this task become?' \n",
    "answer3 = 'passage: the task has become very frustrating'\n",
    "\n",
    "inputs = tokenizer(text=[string3]*3, text_pair = [answer1,answer2,answer3], return_tensors='pt', padding=True)\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "outs = model(**inputs)\n",
    "print([v.item() for v in outs.logits[:,0]] )\n",
    "print(outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BertTokenizerFast in module transformers.models.bert.tokenization_bert_fast object:\n",
      "\n",
      "class BertTokenizerFast(transformers.tokenization_utils_fast.PreTrainedTokenizerFast)\n",
      " |  BertTokenizerFast(vocab_file=None, tokenizer_file=None, do_lower_case=True, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)\n",
      " |  \n",
      " |  Construct a \"fast\" BERT tokenizer (backed by HuggingFace's `tokenizers` library). Based on WordPiece.\n",
      " |  \n",
      " |  This tokenizer inherits from :class:`~transformers.PreTrainedTokenizerFast` which contains most of the main\n",
      " |  methods. Users should refer to this superclass for more information regarding those methods.\n",
      " |  \n",
      " |  Args:\n",
      " |      vocab_file (:obj:`str`):\n",
      " |          File containing the vocabulary.\n",
      " |      do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |          Whether or not to lowercase the input when tokenizing.\n",
      " |      unk_token (:obj:`str`, `optional`, defaults to :obj:`\"[UNK]\"`):\n",
      " |          The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |          token instead.\n",
      " |      sep_token (:obj:`str`, `optional`, defaults to :obj:`\"[SEP]\"`):\n",
      " |          The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
      " |          sequence classification or for a text and a question for question answering. It is also used as the last\n",
      " |          token of a sequence built with special tokens.\n",
      " |      pad_token (:obj:`str`, `optional`, defaults to :obj:`\"[PAD]\"`):\n",
      " |          The token used for padding, for example when batching sequences of different lengths.\n",
      " |      cls_token (:obj:`str`, `optional`, defaults to :obj:`\"[CLS]\"`):\n",
      " |          The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
      " |          instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
      " |      mask_token (:obj:`str`, `optional`, defaults to :obj:`\"[MASK]\"`):\n",
      " |          The token used for masking values. This is the token used when training this model with masked language\n",
      " |          modeling. This is the token which the model will try to predict.\n",
      " |      clean_text (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |          Whether or not to clean the text before tokenization by removing any control characters and replacing all\n",
      " |          whitespaces by the classic one.\n",
      " |      tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |          Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see `this\n",
      " |          issue <https://github.com/huggingface/transformers/issues/328>`__).\n",
      " |      strip_accents: (:obj:`bool`, `optional`):\n",
      " |          Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
      " |          value for :obj:`lowercase` (as in the original BERT).\n",
      " |      wordpieces_prefix: (:obj:`str`, `optional`, defaults to :obj:`\"##\"`):\n",
      " |          The prefix for subwords.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BertTokenizerFast\n",
      " |      transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      transformers.file_utils.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file=None, tokenizer_file=None, do_lower_case=True, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None)\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
      " |      adding special tokens. A BERT sequence has the following format:\n",
      " |      \n",
      " |      - single sequence: ``[CLS] X [SEP]``\n",
      " |      - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs to which the special tokens will be added.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n",
      " |      pair mask has the following format:\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |          0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " |          | first sequence    | second sequence |\n",
      " |      \n",
      " |      If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
      " |          sequence(s).\n",
      " |  \n",
      " |  save_vocabulary(self, save_directory: str, filename_prefix: Union[str, NoneType] = None) -> Tuple[str]\n",
      " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
      " |      \n",
      " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
      " |      :meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save the whole state of the tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |          filename_prefix (:obj:`str`, `optional`):\n",
      " |              An optional prefix to add to the named of the saved files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  max_model_input_sizes = {'TurkuNLP/bert-base-finnish-cased-v1': 512, '...\n",
      " |  \n",
      " |  pretrained_init_configuration = {'TurkuNLP/bert-base-finnish-cased-v1'...\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {'tokenizer_file': {'TurkuNLP/bert-base-f...\n",
      " |  \n",
      " |  slow_tokenizer_class = <class 'transformers.models.bert.tokenization_b...\n",
      " |      Construct a BERT tokenizer. Based on WordPiece.\n",
      " |      \n",
      " |      This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.\n",
      " |      Users should refer to this superclass for more information regarding those methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          vocab_file (:obj:`str`):\n",
      " |              File containing the vocabulary.\n",
      " |          do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to lowercase the input when tokenizing.\n",
      " |          do_basic_tokenize (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to do basic tokenization before WordPiece.\n",
      " |          never_split (:obj:`Iterable`, `optional`):\n",
      " |              Collection of tokens which will never be split during tokenization. Only has an effect when\n",
      " |              :obj:`do_basic_tokenize=True`\n",
      " |          unk_token (:obj:`str`, `optional`, defaults to :obj:`\"[UNK]\"`):\n",
      " |              The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |              token instead.\n",
      " |          sep_token (:obj:`str`, `optional`, defaults to :obj:`\"[SEP]\"`):\n",
      " |              The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
      " |              sequence classification or for a text and a question for question answering. It is also used as the last\n",
      " |              token of a sequence built with special tokens.\n",
      " |          pad_token (:obj:`str`, `optional`, defaults to :obj:`\"[PAD]\"`):\n",
      " |              The token used for padding, for example when batching sequences of different lengths.\n",
      " |          cls_token (:obj:`str`, `optional`, defaults to :obj:`\"[CLS]\"`):\n",
      " |              The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
      " |              instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
      " |          mask_token (:obj:`str`, `optional`, defaults to :obj:`\"[MASK]\"`):\n",
      " |              The token used for masking values. This is the token used when training this model with masked language\n",
      " |              modeling. This is the token which the model will try to predict.\n",
      " |          tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to tokenize Chinese characters.\n",
      " |      \n",
      " |              This should likely be deactivated for Japanese (see this `issue\n",
      " |              <https://github.com/huggingface/transformers/issues/328>`__).\n",
      " |          strip_accents: (:obj:`bool`, `optional`):\n",
      " |              Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
      " |              value for :obj:`lowercase` (as in the original BERT).\n",
      " |  \n",
      " |  vocab_files_names = {'tokenizer_file': 'tokenizer.json', 'vocab_file':...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
      " |      added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`int` or :obj:`List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str` or :obj:`List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (:obj:`str` or :obj:`List[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int` or :obj:`List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens: List[str]) -> str\n",
      " |      Converts a sequence of tokens in a single string. The most simple way to do it is ``\" \".join(tokens)`` but we\n",
      " |      often want to remove sub-word tokenization artifacts at the same time.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (:obj:`List[str]`): The token to join in a string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The joined tokens.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  get_vocab(self) -> Dict[str, int]\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      :obj:`tokenizer.get_vocab()[token]` is equivalent to :obj:`tokenizer.convert_tokens_to_ids(token)` when\n",
      " |      :obj:`token` is in the vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not\n",
      " |          put this inside your training loop.\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  set_truncation_and_padding(self, padding_strategy: transformers.file_utils.PaddingStrategy, truncation_strategy: transformers.tokenization_utils_base.TruncationStrategy, max_length: int, stride: int, pad_to_multiple_of: Union[int, NoneType])\n",
      " |      Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n",
      " |      library) and restore the tokenizer settings afterwards.\n",
      " |      \n",
      " |      The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a\n",
      " |      padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed\n",
      " |      section.\n",
      " |      \n",
      " |      Args:\n",
      " |          padding_strategy (:class:`~transformers.file_utils.PaddingStrategy`):\n",
      " |              The kind of padding that will be applied to the input\n",
      " |          truncation_strategy (:class:`~transformers.tokenization_utils_base.TruncationStrategy`):\n",
      " |              The kind of truncation that will be applied to the input\n",
      " |          max_length (:obj:`int`):\n",
      " |              The maximum size of a sequence.\n",
      " |          stride (:obj:`int`):\n",
      " |              The stride to use when handling overflow.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |  \n",
      " |  tokenize(self, text: str, pair: Union[str, NoneType] = None, add_special_tokens: bool = False, **kwargs) -> List[str]\n",
      " |      Converts a string in a sequence of tokens, replacing unknown tokens with the :obj:`unk_token`.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          pair (:obj:`str`, `optional`):\n",
      " |              A second sequence to be encoded with the first.\n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to add the special tokens associated with the corresponding model.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the underlying model specific encode method. See details in\n",
      " |              :meth:`~transformers.PreTrainedTokenizerBase.__call__`\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of tokens.\n",
      " |  \n",
      " |  train_new_from_iterator(self, text_iterator, vocab_size, new_special_tokens=None, special_tokens_map=None, **kwargs)\n",
      " |      Trains a tokenizer on a new corpus with the same defaults (in terms of special tokens or tokenization pipeline)\n",
      " |      as the current one.\n",
      " |      \n",
      " |      Args:\n",
      " |          text_iterator (generator of :obj:`List[str]`):\n",
      " |              The training corpus. Should be a generator of batches of texts, for instance a list of lists of texts\n",
      " |              if you have everything in memory.\n",
      " |          vocab_size (:obj:`int`):\n",
      " |              The size of the vocabulary you want for your tokenizer.\n",
      " |          new_special_tokens (list of :obj:`str` or :obj:`AddedToken`, `optional`):\n",
      " |              A list of new special tokens to add to the tokenizer you are training.\n",
      " |          special_tokens_map (:obj:`Dict[str, str]`, `optional`):\n",
      " |              If you want to rename some of the special tokens this tokenizer uses, pass along a mapping old special\n",
      " |              token name to new special token name in this argument.\n",
      " |          kwargs:\n",
      " |              Additional keyword arguments passed along to the trainer from the ðŸ¤— Tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`~transformers.PreTrainedTokenizerFast`: A new tokenizer of the same type as the original one,\n",
      " |          trained on :obj:`text_iterator`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  backend_tokenizer\n",
      " |      :obj:`tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.\n",
      " |  \n",
      " |  decoder\n",
      " |      :obj:`tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  vocab_size\n",
      " |      :obj:`int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  __annotations__ = {'can_save_slow_tokenizer': <class 'bool'>, 'slow_to...\n",
      " |  \n",
      " |  can_save_slow_tokenizer = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with :obj:`truncation_strategy = longest_first` or :obj:`True`, an error is\n",
      " |              raised instead of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  as_target_tokenizer(self)\n",
      " |      Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
      " |      sequence-to-sequence models that need a slightly different processing for the labels.\n",
      " |  \n",
      " |  batch_decode(self, sequences: Union[List[int], List[List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (:obj:`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      " |              details in ``encode_plus``).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with :obj:`truncation_strategy = longest_first` or :obj:`True`, an error is\n",
      " |              raised instead of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  decode(self, token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      " |      tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (:obj:`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The decoded sentence.\n",
      " |  \n",
      " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      " |              method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`, :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`: The tokenized ids of the\n",
      " |          text.\n",
      " |  \n",
      " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      " |              method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with :obj:`truncation_strategy = longest_first` or :obj:`True`, an error is\n",
      " |              raised instead of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None, already_has_special_tokens: bool = False) -> List[int]\n",
      " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of ids of the first sequence.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              List of ids of the second sequence.\n",
      " |          already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = True, max_length: Union[int, NoneType] = None, pad_to_multiple_of: Union[int, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with ``self.padding_side``,\n",
      " |      ``self.pad_token_id`` and ``self.pad_token_type_id``)\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |          result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n",
      " |          case of PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs (:class:`~transformers.BatchEncoding`, list of :class:`~transformers.BatchEncoding`, :obj:`Dict[str, List[int]]`, :obj:`Dict[str, List[List[int]]` or :obj:`List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input (:class:`~transformers.BatchEncoding` or :obj:`Dict[str,\n",
      " |              List[int]]`) or a batch of tokenized inputs (list of :class:`~transformers.BatchEncoding`, `Dict[str,\n",
      " |              List[List[int]]]` or `List[Dict[str, List[int]]]`) so you can use this method during preprocessing as\n",
      " |              well as in a PyTorch Dataloader collate function.\n",
      " |      \n",
      " |              Instead of :obj:`List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n",
      " |              see the note above for the return type.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              >= 7.5 (Volta).\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
      " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens. Please Note, for `pair_ids`\n",
      " |      different than `None` and `truncation_strategy = longest_first` or `True`, it is not possible to return\n",
      " |      overflowing tokens. Such a combination of arguments will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with :obj:`truncation_strategy = longest_first` or :obj:`True`, an error is\n",
      " |              raised instead of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Union[List[str], NoneType] = None, max_length: Union[int, NoneType] = None, max_target_length: Union[int, NoneType] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          src_texts (:obj:`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts (:obj:`list`, `optional`):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
      " |              left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
      " |              to :obj:`None`, this will use the max_length value.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to :obj:`self.__call__`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **labels** -- List of token ids for tgt_texts.\n",
      " |      \n",
      " |          The full set of keys ``[input_ids, attention_mask, labels]``, will only be returned if tgt_texts is passed.\n",
      " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  push_to_hub(self, repo_path_or_name: Union[str, NoneType] = None, repo_url: Union[str, NoneType] = None, use_temp_dir: bool = False, commit_message: Union[str, NoneType] = None, organization: Union[str, NoneType] = None, private: Union[bool, NoneType] = None, use_auth_token: Union[bool, str, NoneType] = None) -> str\n",
      " |      Upload the tokenizer files to the ðŸ¤— Model Hub while synchronizing a local clone of the repo in\n",
      " |      :obj:`repo_path_or_name`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_path_or_name (:obj:`str`, `optional`):\n",
      " |              Can either be a repository name for your tokenizer in the Hub or a path to a local folder (in which case\n",
      " |              the repository will have the name of that local folder). If not specified, will default to the name\n",
      " |              given by :obj:`repo_url` and a local directory with that name will be created.\n",
      " |          repo_url (:obj:`str`, `optional`):\n",
      " |              Specify this in case you want to push to an existing repository in the hub. If unspecified, a new\n",
      " |              repository will be created in your namespace (unless you specify an :obj:`organization`) with\n",
      " |              :obj:`repo_name`.\n",
      " |          use_temp_dir (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to clone the distant repo in a temporary directory or in :obj:`repo_path_or_name` inside\n",
      " |              the current working directory. This will slow things down if you are making changes in an existing repo\n",
      " |              since you will need to clone the repo before every push.\n",
      " |          commit_message (:obj:`str`, `optional`):\n",
      " |              Message to commit while pushing. Will default to :obj:`\"add tokenizer\"`.\n",
      " |          organization (:obj:`str`, `optional`):\n",
      " |              Organization in which you want to push your tokenizer (you must be a member of this organization).\n",
      " |          private (:obj:`bool`, `optional`):\n",
      " |              Whether or not the repository created should be private (requires a paying subscription).\n",
      " |          use_auth_token (:obj:`bool` or :obj:`str`, `optional`):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      " |              generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`). Will default to\n",
      " |              :obj:`True` if :obj:`repo_url` is not specified.\n",
      " |      \n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The url of the commit of your tokenizer in the given repository.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          from transformers import AutoTokenizer\n",
      " |      \n",
      " |          tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      \n",
      " |          # Push the tokenizer to your namespace with the name \"my-finetuned-bert\" and have a local clone in the\n",
      " |          # `my-finetuned-bert` folder.\n",
      " |          tokenizer.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |          # Push the tokenizer to your namespace with the name \"my-finetuned-bert\" with no local clone.\n",
      " |          tokenizer.push_to_hub(\"my-finetuned-bert\", use_temp_dir=True)\n",
      " |      \n",
      " |          # Push the tokenizer to an organization with the name \"my-finetuned-bert\" and have a local clone in the\n",
      " |          # `my-finetuned-bert` folder.\n",
      " |          tokenizer.push_to_hub(\"my-finetuned-bert\", organization=\"huggingface\")\n",
      " |      \n",
      " |          # Make a change to an existing repo that has been cloned locally in `my-finetuned-bert`.\n",
      " |          tokenizer.push_to_hub(\"my-finetuned-bert\", repo_url=\"https://huggingface.co/sgugger/my-finetuned-bert\")\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], legacy_format: Union[bool, NoneType] = None, filename_prefix: Union[str, NoneType] = None, push_to_hub: bool = False, **kwargs) -> Tuple[str]\n",
      " |      Save the full tokenizer state.\n",
      " |      \n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.from_pretrained` class method..\n",
      " |      \n",
      " |      .. Warning::\n",
      " |         This won't save modifications you may have applied to the tokenizer after the instantiation (for instance,\n",
      " |         modifying :obj:`tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str` or :obj:`os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
      " |          legacy_format (:obj:`bool`, `optional`):\n",
      " |              Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON\n",
      " |              format as well as in legacy format if it exists, i.e. with tokenizer specific vocabulary and a separate\n",
      " |              added_tokens files.\n",
      " |      \n",
      " |              If :obj:`False`, will only save the tokenizer in the unified JSON format. This format is incompatible\n",
      " |              with \"slow\" tokenizers (not powered by the `tokenizers` library), so the tokenizer will not be able to\n",
      " |              be loaded in the corresponding \"slow\" tokenizer.\n",
      " |      \n",
      " |              If :obj:`True`, will save the tokenizer in legacy format. If the \"slow\" tokenizer doesn't exits, a\n",
      " |              value error is raised.\n",
      " |          filename_prefix: (:obj:`str`, `optional`):\n",
      " |              A prefix to add to the names of the files saved by the tokenizer.\n",
      " |          push_to_hub (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it.\n",
      " |      \n",
      " |              .. warning::\n",
      " |      \n",
      " |                  Using :obj:`push_to_hub=True` will synchronize the repository you are pushing to with\n",
      " |                  :obj:`save_directory`, which requires :obj:`save_directory` to be a local clone of the repo you are\n",
      " |                  pushing to if it's an existing folder. Pass along :obj:`temp_dir=True` to use a temporary directory\n",
      " |                  instead.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of :obj:`str`: The files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          num_tokens_to_remove (:obj:`int`, `optional`, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation_strategy (:obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              * :obj:`'longest_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
      " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[List[int], List[int], List[int]]`: The truncated ``ids``, the truncated ``pair_ids`` and the\n",
      " |          list of overflowing tokens. Note: The `longest_first` strategy returns empty list of overflowing_tokens if\n",
      " |          a pair of sequences (or a batch of pairs) is provided.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\n",
      " |      a predefined tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string, the `model id` of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under a\n",
      " |                user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
      " |              - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`\n",
      " |                method, e.g., ``./my_model_directory/``.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                ``./my_model_directory/vocab.txt``.\n",
      " |          cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (:obj:`Dict[str, str]`, `optional`):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          use_auth_token (:obj:`str` or `bool`, `optional`):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      " |              generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
      " |          local_files_only (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to only rely on local files and not to attempt to download any files.\n",
      " |          revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (:obj:`str`, `optional`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          inputs (additional positional arguments, `optional`):\n",
      " |              Will be passed along to the Tokenizer ``__init__`` method.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like\n",
      " |              ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,\n",
      " |              ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer\n",
      " |          # Download vocabulary from huggingface.co and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
      " |      \n",
      " |          # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      " |      \n",
      " |          # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      " |      \n",
      " |          # You can link tokens to special vocabulary when instantiating\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      " |          # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |          # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |          assert tokenizer.unk_token == '<unk>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string: str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (:obj:`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      :obj:`int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      :obj:`int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  model_input_names = ['input_ids', 'token_type_ids', 'attention_mask']\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      .. Note::\n",
      " |          When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of\n",
      " |          the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |          In order to do that, please use the :meth:`~transformers.PreTrainedModel.resize_token_embeddings` method.\n",
      " |      \n",
      " |      Using :obj:`add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split).\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like :obj:`tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      :class:`~transformers.BertTokenizer` :obj:`cls_token` is already registered to be :obj`'[CLS]'` and XLM's one\n",
      " |      is also registered to be :obj:`'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary `str` to `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``,\n",
      " |              ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
      " |              ``additional_special_tokens``].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to add a new classification token to GPT-2\n",
      " |          tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
      " |          model = GPT2Model.from_pretrained('gpt2')\n",
      " |      \n",
      " |          special_tokens_dict = {'cls_token': '<CLS>'}\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |          assert tokenizer.cls_token == '<CLS>'\n",
      " |  \n",
      " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary.\n",
      " |      \n",
      " |      .. Note::\n",
      " |          When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of\n",
      " |          the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |          In order to do that, please use the :meth:`~transformers.PreTrainedModel.resize_token_embeddings` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (:obj:`str`, :obj:`tokenizers.AddedToken` or a list of `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. :obj:`tokenizers.AddedToken` wraps a\n",
      " |              string token to let you personalize its behavior: whether this token should only match against a single\n",
      " |              word, whether this token should strip all potential whitespaces on the left side, whether this token\n",
      " |              should strip all potential whitespaces on the right side, etc.\n",
      " |          special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for :obj:`tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |          tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
      " |          model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |           # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      Make sure that all the special tokens attributes of the tokenizer (:obj:`tokenizer.mask_token`,\n",
      " |      :obj:`tokenizer.cls_token`, etc.) are in the vocabulary.\n",
      " |      \n",
      " |      Add the missing ones to the vocabulary if needed.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`int`: The number of tokens added in the vocabulary during the operation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      :obj:`List[int]`: List the ids of the special tokens(:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class\n",
      " |      attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      :obj:`List[str]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class attributes.\n",
      " |      \n",
      " |      Convert tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      :obj:`List[Union[str, tokenizers.AddedToken]]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.)\n",
      " |      mapped to class attributes.\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      :obj:`int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      :obj:`Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (:obj:`cls_token`,\n",
      " |      :obj:`unk_token`, etc.) to their values (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      :obj:`Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary\n",
      " |      mapping special token class attributes (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values\n",
      " |      (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      :obj:`List[str]`: All the additional special tokens you may want to use. Log an error if used while not having\n",
      " |      been set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      :obj:`List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  bos_token\n",
      " |      :obj:`str`: Beginning of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns :obj:`None` if the token\n",
      " |      has not been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      :obj:`str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the\n",
      " |      full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      :obj:`Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input\n",
      " |      sequence leveraging self-attention along the full depth of the model.\n",
      " |      \n",
      " |      Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      :obj:`str`: End of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the end of sentence token in the vocabulary. Returns :obj:`None` if the token has\n",
      " |      not been set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      :obj:`str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      :obj:`Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
      " |      modeling. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      :obj:`str`: Padding token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      :obj:`Optional[int]`: Id of the padding token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  sep_token\n",
      " |      :obj:`str`: Separation token, to separate context and query in an input sequence. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      :obj:`Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
      " |      sequence. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  unk_token\n",
      " |      :obj:`str`: Unknown token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      :obj:`Optional[int]`: Id of the unknown token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': {'a': 8}, 'b': {'b': 9}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dicta = {}\n",
    "values = [\"a\",'b']\n",
    "\n",
    "for i,j in enumerate(range(10)):\n",
    "    dicta[values[i%2]] = {}\n",
    "    dicta[values[i%2]][values[j%2]] = j    \n",
    "print(dicta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 8}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
